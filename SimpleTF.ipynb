{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Tensor Flow 2.0 to make a simpe feed forward network running on a toy data, and using Gradient tape for backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The toy data is three different blobs of points spread as a gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "x1_label0 = np.random.normal(1, 1, (100,1))\n",
    "x2_label0 = np.random.normal(1, 1, (100,1))\n",
    "x1_label1 = np.random.normal(5, 1, (100,1))\n",
    "x2_label1 = np.random.normal(4, 1, (100,1))\n",
    "x1_label2 = np.random.normal(8, 1, (100,1))\n",
    "x2_label2 = np.random.normal(0, 1, (100,1))\n",
    "\n",
    "xs_label0 = np.hstack((x1_label0, x2_label0))\n",
    "xs_label1 = np.hstack((x1_label1, x2_label1))\n",
    "xs_label2 = np.hstack((x1_label2, x2_label2))\n",
    "xs = np.vstack((xs_label0, xs_label1, xs_label2))\n",
    "labels = np.matrix([[1.,0.,0.]]*len(x1_label0)+[[0.,1.,0.]]*len(x1_label1)+[[0.,0.,1.]]*len(x2_label2))\n",
    "\n",
    "arr = np.arange(xs.shape[0])\n",
    "np.random.shuffle(arr)\n",
    "xs = xs[arr, :]\n",
    "labels = labels[arr, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x1_label0, x2_label0, 'x')\n",
    "plt.plot(x1_label1, x2_label1, 'o')\n",
    "plt.plot(x1_label2, x2_label2, '_')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size, num_features = xs.shape\n",
    "num_labels = 3\n",
    "hidden_size = 2\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "mini_batch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dense(x, W, b, activation):\n",
    "    return activation(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wh = tf.Variable(np.random.randn(num_features, hidden_size))\n",
    "bh = tf.Variable(np.random.randn(hidden_size))\n",
    "ach = tf.nn.relu\n",
    "\n",
    "Wo = tf.Variable(np.random.randn(hidden_size, num_labels))\n",
    "bo = tf.Variable(np.random.randn(num_labels))\n",
    "aco = tf.nn.softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      "loss at end of EPOCH: 4.66676420592313\n",
      "time taken: 3.66s\n",
      "Done with EPOCH\n",
      "EPOCH 1\n",
      "loss at end of EPOCH: 2.685293991497298\n",
      "time taken: 0.23s\n",
      "Done with EPOCH\n",
      "EPOCH 2\n",
      "loss at end of EPOCH: 1.9574193022850985\n",
      "time taken: 0.22s\n",
      "Done with EPOCH\n",
      "EPOCH 3\n",
      "loss at end of EPOCH: 1.568502035961147\n",
      "time taken: 0.22s\n",
      "Done with EPOCH\n",
      "EPOCH 4\n",
      "loss at end of EPOCH: 1.2838349501332416\n",
      "time taken: 0.24s\n",
      "Done with EPOCH\n",
      "EPOCH 5\n",
      "loss at end of EPOCH: 1.0567982332509342\n",
      "time taken: 0.27s\n",
      "Done with EPOCH\n",
      "EPOCH 6\n",
      "loss at end of EPOCH: 0.8774572923983797\n",
      "time taken: 0.23s\n",
      "Done with EPOCH\n",
      "EPOCH 7\n",
      "loss at end of EPOCH: 0.7390653971089624\n",
      "time taken: 0.20s\n",
      "Done with EPOCH\n",
      "EPOCH 8\n",
      "loss at end of EPOCH: 0.6361638409329212\n",
      "time taken: 0.25s\n",
      "Done with EPOCH\n",
      "EPOCH 9\n",
      "loss at end of EPOCH: 0.5466095345575687\n",
      "time taken: 0.25s\n",
      "Done with EPOCH\n",
      "EPOCH 10\n",
      "loss at end of EPOCH: 0.4759186620685819\n",
      "time taken: 0.24s\n",
      "Done with EPOCH\n",
      "EPOCH 11\n",
      "loss at end of EPOCH: 0.4172409931239781\n",
      "time taken: 0.21s\n",
      "Done with EPOCH\n",
      "EPOCH 12\n",
      "loss at end of EPOCH: 0.3715266160861469\n",
      "time taken: 0.24s\n",
      "Done with EPOCH\n",
      "EPOCH 13\n",
      "loss at end of EPOCH: 0.3268540735047\n",
      "time taken: 0.20s\n",
      "Done with EPOCH\n",
      "EPOCH 14\n",
      "loss at end of EPOCH: 0.2889636515657301\n",
      "time taken: 0.23s\n",
      "Done with EPOCH\n",
      "EPOCH 15\n",
      "loss at end of EPOCH: 0.2566521191006904\n",
      "time taken: 0.20s\n",
      "Done with EPOCH\n",
      "EPOCH 16\n",
      "loss at end of EPOCH: 0.22903080296934028\n",
      "time taken: 0.24s\n",
      "Done with EPOCH\n",
      "EPOCH 17\n",
      "loss at end of EPOCH: 0.20563633583730256\n",
      "time taken: 0.22s\n",
      "Done with EPOCH\n",
      "EPOCH 18\n",
      "loss at end of EPOCH: 0.18633878077108657\n",
      "time taken: 0.23s\n",
      "Done with EPOCH\n",
      "EPOCH 19\n",
      "loss at end of EPOCH: 0.16958648107939173\n",
      "time taken: 0.21s\n",
      "Done with EPOCH\n",
      "EPOCH 20\n",
      "loss at end of EPOCH: 0.15536341407657112\n",
      "time taken: 0.23s\n",
      "Done with EPOCH\n",
      "EPOCH 21\n",
      "loss at end of EPOCH: 0.14291344767992292\n",
      "time taken: 0.20s\n",
      "Done with EPOCH\n",
      "EPOCH 22\n",
      "loss at end of EPOCH: 0.13208119657093717\n",
      "time taken: 0.23s\n",
      "Done with EPOCH\n",
      "EPOCH 23\n",
      "loss at end of EPOCH: 0.12263944837324608\n",
      "time taken: 0.26s\n",
      "Done with EPOCH\n",
      "EPOCH 24\n",
      "loss at end of EPOCH: 0.11437675358398332\n",
      "time taken: 0.25s\n",
      "Done with EPOCH\n",
      "EPOCH 25\n",
      "loss at end of EPOCH: 0.1072727278416016\n",
      "time taken: 0.25s\n",
      "Done with EPOCH\n",
      "EPOCH 26\n",
      "loss at end of EPOCH: 0.10092369281385714\n",
      "time taken: 0.21s\n",
      "Done with EPOCH\n",
      "EPOCH 27\n",
      "loss at end of EPOCH: 0.09531285395931934\n",
      "time taken: 0.24s\n",
      "Done with EPOCH\n",
      "EPOCH 28\n",
      "loss at end of EPOCH: 0.09010898848237951\n",
      "time taken: 0.24s\n",
      "Done with EPOCH\n",
      "EPOCH 29\n",
      "loss at end of EPOCH: 0.08544713241480749\n",
      "time taken: 0.24s\n",
      "Done with EPOCH\n",
      "EPOCH 30\n",
      "loss at end of EPOCH: 0.08123950487342732\n",
      "time taken: 0.24s\n",
      "Done with EPOCH\n",
      "EPOCH 31\n",
      "loss at end of EPOCH: 0.07737995330520689\n",
      "time taken: 0.25s\n",
      "Done with EPOCH\n",
      "EPOCH 32\n",
      "loss at end of EPOCH: 0.07392384113516447\n",
      "time taken: 0.24s\n",
      "Done with EPOCH\n",
      "EPOCH 33\n",
      "loss at end of EPOCH: 0.07076728047365974\n",
      "time taken: 0.24s\n",
      "Done with EPOCH\n",
      "EPOCH 34\n",
      "loss at end of EPOCH: 0.06787083428361575\n",
      "time taken: 0.22s\n",
      "Done with EPOCH\n",
      "EPOCH 35\n",
      "loss at end of EPOCH: 0.06520186742684325\n",
      "time taken: 0.23s\n",
      "Done with EPOCH\n",
      "EPOCH 36\n",
      "loss at end of EPOCH: 0.06273415362748119\n",
      "time taken: 0.22s\n",
      "Done with EPOCH\n",
      "EPOCH 37\n",
      "loss at end of EPOCH: 0.0604459771556987\n",
      "time taken: 0.24s\n",
      "Done with EPOCH\n",
      "EPOCH 38\n",
      "loss at end of EPOCH: 0.05832296749716122\n",
      "time taken: 0.22s\n",
      "Done with EPOCH\n",
      "EPOCH 39\n",
      "loss at end of EPOCH: 0.05633749584868843\n",
      "time taken: 0.23s\n",
      "Done with EPOCH\n",
      "EPOCH 40\n",
      "loss at end of EPOCH: 0.05448906566598202\n",
      "time taken: 0.23s\n",
      "Done with EPOCH\n",
      "EPOCH 41\n",
      "loss at end of EPOCH: 0.05276231008253293\n",
      "time taken: 0.23s\n",
      "Done with EPOCH\n",
      "EPOCH 42\n",
      "loss at end of EPOCH: 0.05114542245999492\n",
      "time taken: 0.22s\n",
      "Done with EPOCH\n",
      "EPOCH 43\n",
      "loss at end of EPOCH: 0.04962843673688774\n",
      "time taken: 0.23s\n",
      "Done with EPOCH\n",
      "EPOCH 44\n",
      "loss at end of EPOCH: 0.04820274070796951\n",
      "time taken: 0.24s\n",
      "Done with EPOCH\n",
      "EPOCH 45\n",
      "loss at end of EPOCH: 0.046860771828185294\n",
      "time taken: 0.24s\n",
      "Done with EPOCH\n",
      "EPOCH 46\n",
      "loss at end of EPOCH: 0.04559581591769521\n",
      "time taken: 0.24s\n",
      "Done with EPOCH\n",
      "EPOCH 47\n",
      "loss at end of EPOCH: 0.044401866186310826\n",
      "time taken: 0.20s\n",
      "Done with EPOCH\n",
      "EPOCH 48\n",
      "loss at end of EPOCH: 0.04327351897095563\n",
      "time taken: 0.25s\n",
      "Done with EPOCH\n",
      "EPOCH 49\n",
      "loss at end of EPOCH: 0.04220589289419654\n",
      "time taken: 0.23s\n",
      "Done with EPOCH\n",
      "EPOCH 50\n",
      "loss at end of EPOCH: 0.041194786685864716\n",
      "time taken: 0.20s\n",
      "Done with EPOCH\n",
      "EPOCH 51\n",
      "loss at end of EPOCH: 0.04023586132564199\n",
      "time taken: 0.25s\n",
      "Done with EPOCH\n",
      "EPOCH 52\n",
      "loss at end of EPOCH: 0.03932551891498951\n",
      "time taken: 0.23s\n",
      "Done with EPOCH\n",
      "EPOCH 53\n",
      "loss at end of EPOCH: 0.03846064671884404\n",
      "time taken: 0.22s\n",
      "Done with EPOCH\n",
      "EPOCH 54\n",
      "loss at end of EPOCH: 0.0376378449768404\n",
      "time taken: 0.22s\n",
      "Done with EPOCH\n",
      "EPOCH 55\n",
      "loss at end of EPOCH: 0.036854410237847836\n",
      "time taken: 0.24s\n",
      "Done with EPOCH\n",
      "EPOCH 56\n",
      "loss at end of EPOCH: 0.036107785865412495\n",
      "time taken: 0.24s\n",
      "Done with EPOCH\n",
      "EPOCH 57\n",
      "loss at end of EPOCH: 0.03539628953238892\n",
      "time taken: 0.25s\n",
      "Done with EPOCH\n",
      "EPOCH 58\n",
      "loss at end of EPOCH: 0.03471642472333146\n",
      "time taken: 0.24s\n",
      "Done with EPOCH\n",
      "EPOCH 59\n",
      "loss at end of EPOCH: 0.03406700622891897\n",
      "time taken: 0.22s\n",
      "Done with EPOCH\n",
      "EPOCH 60\n",
      "loss at end of EPOCH: 0.03344611584354952\n",
      "time taken: 0.21s\n",
      "Done with EPOCH\n",
      "EPOCH 61\n",
      "loss at end of EPOCH: 0.03285194444526877\n",
      "time taken: 0.21s\n",
      "Done with EPOCH\n",
      "EPOCH 62\n",
      "loss at end of EPOCH: 0.032282882397940166\n",
      "time taken: 0.23s\n",
      "Done with EPOCH\n",
      "EPOCH 63\n",
      "loss at end of EPOCH: 0.0317374524064324\n",
      "time taken: 0.32s\n",
      "Done with EPOCH\n",
      "EPOCH 64\n",
      "loss at end of EPOCH: 0.03121428727392819\n",
      "time taken: 0.31s\n",
      "Done with EPOCH\n",
      "EPOCH 65\n",
      "loss at end of EPOCH: 0.03071211837570971\n",
      "time taken: 0.24s\n",
      "Done with EPOCH\n",
      "EPOCH 66\n",
      "loss at end of EPOCH: 0.03022976643092151\n",
      "time taken: 0.41s\n",
      "Done with EPOCH\n",
      "EPOCH 67\n",
      "loss at end of EPOCH: 0.029766856386520123\n",
      "time taken: 0.40s\n",
      "Done with EPOCH\n",
      "EPOCH 68\n",
      "loss at end of EPOCH: 0.029320767821074685\n",
      "time taken: 0.33s\n",
      "Done with EPOCH\n",
      "EPOCH 69\n",
      "loss at end of EPOCH: 0.028891601784184928\n",
      "time taken: 0.25s\n",
      "Done with EPOCH\n",
      "EPOCH 70\n",
      "loss at end of EPOCH: 0.02847837413834961\n",
      "time taken: 0.30s\n",
      "Done with EPOCH\n",
      "EPOCH 71\n",
      "loss at end of EPOCH: 0.028080088862116204\n",
      "time taken: 0.40s\n",
      "Done with EPOCH\n",
      "EPOCH 72\n",
      "loss at end of EPOCH: 0.02769630332636615\n",
      "time taken: 0.31s\n",
      "Done with EPOCH\n",
      "EPOCH 73\n",
      "loss at end of EPOCH: 0.027326104882744263\n",
      "time taken: 0.29s\n",
      "Done with EPOCH\n",
      "EPOCH 74\n",
      "loss at end of EPOCH: 0.02696874902626315\n",
      "time taken: 0.41s\n",
      "Done with EPOCH\n",
      "EPOCH 75\n",
      "loss at end of EPOCH: 0.026623568136675452\n",
      "time taken: 0.26s\n",
      "Done with EPOCH\n",
      "EPOCH 76\n",
      "loss at end of EPOCH: 0.02629014494736122\n",
      "time taken: 0.28s\n",
      "Done with EPOCH\n",
      "EPOCH 77\n",
      "loss at end of EPOCH: 0.02596757924892793\n",
      "time taken: 0.24s\n",
      "Done with EPOCH\n",
      "EPOCH 78\n",
      "loss at end of EPOCH: 0.025655580564741882\n",
      "time taken: 0.24s\n",
      "Done with EPOCH\n",
      "EPOCH 79\n",
      "loss at end of EPOCH: 0.02535358138187647\n",
      "time taken: 0.23s\n",
      "Done with EPOCH\n",
      "EPOCH 80\n",
      "loss at end of EPOCH: 0.025061081085922282\n",
      "time taken: 0.22s\n",
      "Done with EPOCH\n",
      "EPOCH 81\n",
      "loss at end of EPOCH: 0.024777624872657826\n",
      "time taken: 0.24s\n",
      "Done with EPOCH\n",
      "EPOCH 82\n",
      "loss at end of EPOCH: 0.024501258954801097\n",
      "time taken: 0.24s\n",
      "Done with EPOCH\n",
      "EPOCH 83\n",
      "loss at end of EPOCH: 0.024235911277842033\n",
      "time taken: 0.40s\n",
      "Done with EPOCH\n",
      "EPOCH 84\n",
      "loss at end of EPOCH: 0.023979645799166935\n",
      "time taken: 0.21s\n",
      "Done with EPOCH\n",
      "EPOCH 85\n",
      "loss at end of EPOCH: 0.02373133782176274\n",
      "time taken: 0.25s\n",
      "Done with EPOCH\n",
      "EPOCH 86\n",
      "loss at end of EPOCH: 0.023490404153345003\n",
      "time taken: 0.24s\n",
      "Done with EPOCH\n",
      "EPOCH 87\n",
      "loss at end of EPOCH: 0.02325638577556094\n",
      "time taken: 0.23s\n",
      "Done with EPOCH\n",
      "EPOCH 88\n",
      "loss at end of EPOCH: 0.023028918867140504\n",
      "time taken: 0.24s\n",
      "Done with EPOCH\n",
      "EPOCH 89\n",
      "loss at end of EPOCH: 0.022807691971477924\n",
      "time taken: 0.36s\n",
      "Done with EPOCH\n",
      "EPOCH 90\n",
      "loss at end of EPOCH: 0.02259242561442377\n",
      "time taken: 0.37s\n",
      "Done with EPOCH\n",
      "EPOCH 91\n",
      "loss at end of EPOCH: 0.022382862429917917\n",
      "time taken: 0.38s\n",
      "Done with EPOCH\n",
      "EPOCH 92\n",
      "loss at end of EPOCH: 0.022178762144949286\n",
      "time taken: 0.26s\n",
      "Done with EPOCH\n",
      "EPOCH 93\n",
      "loss at end of EPOCH: 0.021979898828219827\n",
      "time taken: 0.25s\n",
      "Done with EPOCH\n",
      "EPOCH 94\n",
      "loss at end of EPOCH: 0.021786059210054852\n",
      "time taken: 0.25s\n",
      "Done with EPOCH\n",
      "EPOCH 95\n",
      "loss at end of EPOCH: 0.02159704152487566\n",
      "time taken: 0.24s\n",
      "Done with EPOCH\n",
      "EPOCH 96\n",
      "loss at end of EPOCH: 0.02141265462253156\n",
      "time taken: 0.26s\n",
      "Done with EPOCH\n",
      "EPOCH 97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at end of EPOCH: 0.02123284432476672\n",
      "time taken: 0.39s\n",
      "Done with EPOCH\n",
      "EPOCH 98\n",
      "loss at end of EPOCH: 0.021057187299612964\n",
      "time taken: 0.39s\n",
      "Done with EPOCH\n",
      "EPOCH 99\n",
      "loss at end of EPOCH: 0.02088566125470299\n",
      "time taken: 0.30s\n",
      "Done with EPOCH\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    print(\"EPOCH\", i)\n",
    "    start = time.time()\n",
    "    for j in range(0, train_size, mini_batch):\n",
    "        x = xs[j:j+mini_batch, :]\n",
    "        y = labels[j:j+mini_batch, :]\n",
    "        with tf.GradientTape(persistent=True, watch_accessed_variables=False) as t:\n",
    "            t.watch([Wh, bh, Wo, bo])\n",
    "            hidden = Dense(x, Wh, bh, ach)\n",
    "            out = Dense(hidden, Wo, bo, aco)\n",
    "            loss = -tf.reduce_sum(y*tf.math.log(out))\n",
    "        Wh.assign_sub(learning_rate*t.gradient(loss,Wh))\n",
    "        bh.assign_sub(learning_rate*t.gradient(loss,bh))\n",
    "        Wo.assign_sub(learning_rate*t.gradient(loss,Wo))\n",
    "        bo.assign_sub(learning_rate*t.gradient(loss,bo))\n",
    "        del t\n",
    "    end = time.time()\n",
    "    print(\"loss at end of EPOCH:\", loss.numpy())\n",
    "    print(\"time taken: {:0.2f}s\".format(end-start))\n",
    "    print(\"Done with EPOCH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "y_model = Dense(Dense(xs, Wh, bh, ach), Wo, bo, aco)\n",
    "correct_prediction = tf.equal(tf.argmax(y_model, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print(\"Final training accuracy:\", accuracy.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "test_x1_label0 = np.random.normal(1, 1, (10,1))\n",
    "test_x2_label0 = np.random.normal(1, 1, (10,1))\n",
    "test_x1_label1 = np.random.normal(5, 1, (10,1))\n",
    "test_x2_label1 = np.random.normal(4, 1, (10,1))\n",
    "test_x1_label2 = np.random.normal(8, 1, (10,1))\n",
    "test_x2_label2 = np.random.normal(0, 1, (10,1))\n",
    "test_xs_label0 = np.hstack((test_x1_label0, test_x2_label0))\n",
    "test_xs_label1 = np.hstack((test_x1_label1, test_x2_label1))\n",
    "test_xs_label2 = np.hstack((test_x1_label2, test_x2_label2))\n",
    "test_xs = np.vstack((test_xs_label0, test_xs_label1, test_xs_label2))\n",
    "test_labels = np.matrix([[1.,0.,0.]]*10+[[0.,1.,0.]]*10+[[0.,0.,1.]]*10)\n",
    "\n",
    "y_test = Dense(Dense(test_xs, Wh, bh, ach), Wo, bo, aco)\n",
    "correct_prediction = tf.equal(tf.argmax(y_test, 1), tf.argmax(test_labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print(\"Final test accuracy:\", accuracy.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
